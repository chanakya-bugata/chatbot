{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548a8313-5077-44eb-b7eb-5bcbd521fecb",
   "metadata": {},
   "source": [
    "# Open Terminal\n",
    "Run the following commands\n",
    "\n",
    "conda create -n itrex python=3.10 -y\n",
    "conda activate itrex\n",
    "\n",
    "pip install intel-extension-for-transformers\n",
    "\n",
    "git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "\n",
    "cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "\n",
    "pip install -r requirements_cpu.txt\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "huggingface-cli login\n",
    "(enter your hugging face login details)\n",
    "\n",
    "##install jupyter and ipykernel \n",
    "python3 -m pip install jupyter ipykernel\n",
    "\n",
    "##Add kernel for its environment \n",
    "python3 -m ipykernel install --name Neural-Chat --user\n",
    "\n",
    "In the notebook set kernel to Neural-Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe77b7-42f4-40cb-b3db-2b59a1627cbf",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedb179-3f73-4496-9723-93bd9e69e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e916c-dab2-4cbd-9980-fe335aa250df",
   "metadata": {},
   "source": [
    "## Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec9bb3-9475-4530-8bee-13dffe6ac34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "model_args = ModelArguments(model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "data_args = DataArguments(train_file=\"alpaca_data.json\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./finetuned_model_path',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "finetune_args = FinetuningArguments()\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "            model_args=model_args,\n",
    "            data_args=data_args,\n",
    "            training_args=training_args,\n",
    "            finetune_args=finetune_args,\n",
    "        )\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d6e9c-1147-4a25-b419-0b7c2194dad4",
   "metadata": {},
   "source": [
    "## BF16 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f7ecd-d0e6-45d9-88cb-b39f1ea923a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BF16 Optimization\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb009d-8007-45ed-bf14-c81233f98dab",
   "metadata": {},
   "source": [
    "## Text Chat With Retrieval Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a50df-bb9e-476c-b5cf-167f332e331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.retrieval.enable=False\n",
    "plugins.retrieval.args[\"input_path\"]=\"./docs/\"\n",
    "config = PipelineConfig(plugins=plugins)\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(\"How many cores does the Intel® Xeon® Platinum 8480+ Processor have in total?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b149d-be9f-454c-8eca-f96eee8afcb3",
   "metadata": {},
   "source": [
    "## Setup Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca53ccc-cb1c-4917-b0b4-94879c6f3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers\n",
    "!git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt\n",
    "!sudo apt install numactl\n",
    "!conda install astunparse ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses -y\n",
    "!conda install jemalloc gperftools -c conda-forge -y\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04736ae0-5fb6-44e6-96ca-a1c6e0ee227a",
   "metadata": {},
   "source": [
    "## Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8c5ae-4d97-4c10-846e-bf50be31e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/examples/deployment/textbot/backend/xeon/textbot.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9a58a-6bc3-4892-a8d4-b1da446bf4bf",
   "metadata": {},
   "source": [
    "## Setup frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477b460-4e58-4997-a0d3-07c09d594c86",
   "metadata": {},
   "source": [
    "Hugging Face Space helps to make some amazing ML applications more accessible to the community. Inspired by this, we can create a chatbot application on Hugging Face Spaces. Alternatively, you can also deploy the frontend on your own server.\n",
    "\n",
    "## Deploy on Huggingface Space\n",
    "\n",
    "### Create a new space on Huggingface\n",
    "To create a new application space on Hugging Face, visit the website at [https://huggingface.co/new-space](https://huggingface.co/new-space) and follow the below steps to create a new space.\n",
    "\n",
    "![Create New Space](https://i.imgur.com/QyjqUd6.png)\n",
    "\n",
    "The new space is like a new project that supports GitHub-style code repository management.\n",
    "\n",
    "### Check configuration\n",
    "We recommend using Gradio as the Space SDK, keeping the default values for the other settings.\n",
    "\n",
    "For detailed information about the configuration settings, please refer to the [Hugging Face Spaces Config Reference](https://huggingface.co/docs/hub/spaces-config-reference).\n",
    "\n",
    "### Setup application\n",
    "We strongly recommend utilizing the provided textbot frontend code as it represents the reference implementation already deployed on Hugging Face Space. To establish your application, simply copy the code files from this directory(intel_extension_for_transformers/neural_chat/examples/textbot/frontend) and adjust their configurations as necessary (e.g., backend service URL in the `app.py` file like below).\n",
    "\n",
    "![Update backend URL](https://i.imgur.com/rQxPOV7.png)\n",
    "\n",
    "Alternatively, you have the option to clone the existing space from [https://huggingface.co/spaces/Intel/NeuralChat-GNR-1](https://huggingface.co/spaces/Intel/NeuralChat-GNR-1).\n",
    "\n",
    "![Clone Space](https://i.imgur.com/76N8m5B.png)\n",
    "\n",
    "Please also update the backend service URL in the `app.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb8431-928b-4e65-b834-fb9f308ed515",
   "metadata": {},
   "source": [
    "## Deploy frontend on your server\n",
    "\n",
    "### Install the required Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37497260-ca43-4550-98d1-d3cf79a11b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./examples/deployment/textbot/frontend/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbae24e-ee9f-45ad-8af8-1d5b89953514",
   "metadata": {},
   "source": [
    "# Run the frontend\n",
    "## Launch the chatbot frontend on your server using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7cac03-fbf5-4778-9bda-d8ee72f54aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./examples/deployment/textbot/frontend/\n",
    "!nohup python app.py &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54a054-beda-4fab-b08f-35ed4ace07de",
   "metadata": {},
   "source": [
    "This will run the chatbot application in the background on your server. The port is defined in `server_port=` at the end of the `app.py` file.\n",
    "\n",
    "Once the application is running, you can find the access URL in the trace log:\n",
    "\n",
    "```log\n",
    "INFO | gradio_web_server | Models: meta-llama/Llama-2-7b-chat-hf\n",
    "INFO | stdout | Running on local URL:  http://0.0.0.0:7860\n",
    "```\n",
    "The URL to access the chatbot frontend is http://SERVER_IP_ADDRESS:7860. Please remember to replace SERVER_IP_ADDRESS with your server's actual IP address.\n",
    "\n",
    "![URL](https://i.imgur.com/La3tJ8d.png)\n",
    "\n",
    "Please update the backend service URL in the `app.py` file.\n",
    "\n",
    "![Update backend URL](https://i.imgur.com/gRtZHrJ.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neutral-Chat",
   "language": "python",
   "name": "neutral-chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
